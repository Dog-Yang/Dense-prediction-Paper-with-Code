# paper-list
- [Classical segmentation methods](#Classical)
- [ViT sementation](#ViT)
- [Open vocabulary segmentation](#Open)
- [Other Technologies](#Other)

<a name="Classical"></a>
# Classical segmentation method
**FCN: Fully convolutional networks (CVPR'2015)**
- Paper: https://arxiv.org/abs/1411.4038
- Code: https://github.com/BVLC/caffe/wiki/Model-Zoo#fcn

**UNet (MICCAI'2016)**
- Paper: https://arxiv.org/pdf/1505.04597

**DeepLabV3：Rethinking atrous convolution for semantic image segmentation (ArXiv'2017)**
- Paper: https://arxiv.org/pdf/1706.05587
- Contribution：Atrous Convolution(Dilated Convolution)

**DeepLabV3+ ：Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation (CVPR'2018)**
- Paper: https://arxiv.org/pdf/1802.02611
- Contribution：The convolution layer and pooling layer is replaced by a depth-separable convolution

**Semantic FPN ：Panoptic Feature Pyramid Networks(CVPR'2019)**
- Paper: https://arxiv.org/pdf/1901.02446
- Contribution：Panoptic Feature Pyramid Networks


**BiSeNetV2 (IJCV'2021)**
**STDC (CVPR'2021)**
**SETR (CVPR'2021)**
**DPT (ArXiv'2021)**
**Segmenter (ICCV'2021)**
- Paper: https://arxiv.org/pdf/1901.02446
- Code: 
- Contribution：Supervised ViT Segmentation，ViT Enconder + ViT Decoder

**SegFormer (NeurIPS'2021)**
**K-Net (NeurIPS'2021)**
**DEST (CVPRW'2022)**

<a name="ViT"></a>
# ViT sementation
**DeiT:Training data-efficient image transformers & distillation through attention**
- Paper: https://arxiv.org/pdf/2012.12877
- Contribution：Distillation Token；Teacher Model:CNN, Student model: ViT

**Swin Transfomer:**
- Paper: https://arxiv.org/pdf/2103.14030
- Code: https://github.com/microsoft/Swin-Transformer
- Contribution：Sliding Window Attention + Patch Merging


- Paper: 
- Code: 
- Contribution：


<a name="Open"></a>
# Open vocabulary segmentation


<a name="Other"></a>
# Other Technologies
**pixel shuffle**
- Paper: https://arxiv.org/pdf/1609.05158
